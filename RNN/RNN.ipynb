{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h1>This is Implementing One Layer Recurrent Neural Network from Scratch</h1>",
   "id": "2952ab189c7e73f0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The hidden state is computed as\n",
    "$\\large h_t = \\tanh(W_{hh} h_{t-1} + W_{xh} x_t + b_h)$.\n",
    "\n",
    "The output is given by\n",
    "$\\large y_t = W_{hy} h_t + b_y$.\n"
   ],
   "id": "62f67c804c6d667e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "so remember hidden state is really the RNN part of this, that yt a linear layer\n",
    "in pytorch what nn.RNN would return is that ht on the last layer of the RNN network. we put linear\n",
    "layer so that we can make decisions. just like in CNN, CNN part extract data and Linear layer do the\n",
    "rest as we say. In RNN also ht remember what's necessary and linear layer do what we want.\n",
    "(classification or regression)"
   ],
   "id": "4b0781faa72e576d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<h3>So here we first see how is forward pass works </h3>\n",
    "input size is X = (seq_len,batch_size,input_size) , so in the network input features first multiplied\n",
    "by Wxh . so its like (seq_len,batch_size,features) * (input_size,hidden_size) . so this gives the\n",
    "(seq_len,batch_size,hidden_size) . but we cant use this architecture like this. if you think about\n",
    "doing simple matrix multiplication it will broadcast same hidden state for all sequence, and it is\n",
    "wrong since we need to use previous hidden state for to calculate current hidden state. so that why\n",
    "we use hidden state.\n",
    "\n",
    "And we generate random numbers as the transpose to (input_size, hidden_size) meaning\n",
    "(hidden_size,input_size), just like pytorch so that multiplication will be easy. if not we have to\n",
    "transpose that again.\n",
    "\n",
    "So about hidden state and for loop, in the for loop we take one seq at time that give the size of\n",
    "(batch_size,input_size) then we transpose it (input_size,batch_size) so we can do multiplication\n",
    "like this Whx @ X .which gives size of (hidden_size, batch_size) . when you think about it , it is\n",
    "all the hidden states of each batch member. that why h = (hidden_size, batch_size) . so I think if i\n",
    "ever forget , it will be this. so after tha we have to calculate Whh @ h and then add all with bias bh\n",
    ". so we store those hidden states one by one , and new hidden state got calculated by last one.\n",
    "that's the forward pass.\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "98fda89397e47db6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<h3>Back propagation </h3>\n",
    "About back prop. you have to do the calculations by hand , and i dont know latex that much to write\n",
    "those here. so the way you have to approach is , write equation with most abstract way . like\n",
    "Loss = 1/2(yp - y)**2 , then calculate the derivative respect to all things except constants . Its\n",
    "chain rule, but what we usually do is in the class is find most simplified version\n",
    "of the equation the getting the derivative of each one. but we cant do that here it will get messy\n",
    "coding. so do that for each weight and bias. then multiply with previous one .\n",
    "\n",
    "\n",
    "the new thing is this dhlast, This will be hard to explain. think about ht, if the ht is last hidden\n",
    "state that generated this ht cause two errors. first by current output yp, and the error that this\n",
    "for h(t+1) . (this is by this why it called back propagation through time, which does not happen\n",
    "other networks) dlast is the gradient of that how much error he contributed to next h(t+1). but in\n",
    "here h is the last hidden state. So , h doesn't cause error yet. but h(t-1) have cause error to ht.\n",
    "that why we start with zeros . And remember we are going back in time with this. we calculate all the\n",
    "errors from last point to first point, that it generated. And we add that error to Whh. since Whh\n",
    "responsible for generating ht from h(t-1)."
   ],
   "id": "d21fcabd1849cbe9"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-29T17:07:43.043056Z",
     "start_time": "2025-01-29T17:07:43.034504Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n"
   ],
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T17:09:22.954401Z",
     "start_time": "2025-01-29T17:09:22.936420Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class RNN():\n",
    "    def __init__(self,input_size,hidden_sate_size,output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_state_size = hidden_sate_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.Whx = np.random.randn(self.hidden_state_size,self.input_size)/np.sqrt(self.hidden_state_size)\n",
    "        self.Whh = np.random.randn(self.hidden_state_size,self.hidden_state_size)/np.sqrt(self.hidden_state_size)\n",
    "        self.Wyh = np.random.randn(output_size, self.hidden_state_size) / np.sqrt(self.hidden_state_size)\n",
    "\n",
    "        self.bh = np.zeros((self.hidden_state_size, 1))\n",
    "        self.by = np.zeros((self.output_size, 1))\n",
    "\n",
    "        self.inputs = None\n",
    "        self.hidden_states = None\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs #(seq,batch,input)\n",
    "        seq_len,batch_size,features = inputs.shape\n",
    "\n",
    "        h = np.zeros((self.hidden_state_size,batch_size))\n",
    "        self.hidden_states = [h]\n",
    "        outputs = np.zeros((seq_len, batch_size, self.output_size))\n",
    "\n",
    "        for t,x in enumerate(inputs):\n",
    "            x = x.T #(input,batch)\n",
    "            h = np.tanh(\n",
    "                np.dot(self.Whh,h) + #(hidden,hidden) * (hidden,batch) = (hidden,batch)\n",
    "                np.dot(self.Whx,x) + #(hidden,input) * (input,batch) = (hidden,batch)\n",
    "                self.bh              #(hidden,1) column vector is going through all batches\n",
    "            )\n",
    "            y = np.dot(self.Wyh,h) + self.by #(output,hidden) * (hidden,output) = (output *batch)\n",
    "            outputs[t] = y.T # (batch,output)\n",
    "            self.hidden_states.append(h)\n",
    "\n",
    "        return outputs # (seq,batch,input)\n",
    "\n",
    "    def backward(self, outputs, targets, learning_rate=0.01):\n",
    "\n",
    "        _, batch_size, _ = outputs.shape #(seq,batch,input)\n",
    "\n",
    "        dWhx = np.zeros_like(self.Whx) # (hidden, input)\n",
    "        dWhh = np.zeros_like(self.Whh) #(hidden,hidden)\n",
    "        dWhy = np.zeros_like(self.Wyh) #(output * hidden)\n",
    "        dbh = np.zeros_like(self.bh) # (hidden,1)\n",
    "        dby = np.zeros_like(self.by) #(output,1)\n",
    "\n",
    "        dhlast = np.zeros((self.hidden_state_size, batch_size))\n",
    "\n",
    "        for t in reversed(range(len(outputs))):\n",
    "\n",
    "            dy = outputs[t].T - targets[t].T #(output,batch)\n",
    "            dWhy += np.dot(dy, self.hidden_states[t].T) #(output,batch) * (batch,hidden) = (output,hidden)\n",
    "            dby += np.sum(dy, axis=1, keepdims=True) # sum of all dy in column wise\n",
    "\n",
    "            dh = np.dot(self.Wyh.T, dy) + dhlast #(hidden,output) * (output,batch) = (hidden,batch)\n",
    "            dtanh = (1 - self.hidden_states[t] ** 2) * dh #(hidden,batch)\n",
    "            dbh += np.sum(dtanh, axis=1, keepdims=True)  # sum of all dtanh in column wise\n",
    "            dWhx += np.dot(dtanh, self.inputs[t])  #(hidden,batch) * (batch,inputs) = (hidden * input)\n",
    "            dWhh += np.dot(dtanh, self.hidden_states[t-1].T) # (hidden,batch) * (batch,hidden) = (hidden * hidden)\n",
    "            dhlast = np.dot(self.Whh.T, dtanh) #(hidden,hidden) * (hidden,batch) = (hidden,batch)\n",
    "\n",
    "        self.Whx -= learning_rate * dWhx\n",
    "        self.Whh -= learning_rate * dWhh\n",
    "        self.Wyh -= learning_rate * dWhy\n",
    "        self.bh -= learning_rate * dbh\n",
    "        self.by -= learning_rate * dby\n",
    "\n"
   ],
   "id": "64a411c57e9ef100",
   "outputs": [],
   "execution_count": 52
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
